{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned K Value:  3\n",
      "\n",
      "Validation Dataset Accuray:  77.0\n",
      "Test Dataset Accuracy:  71.02803738317756\n",
      "\n",
      "L1 Accuracy:  0.7827102803738317\n",
      "L1 Precision:  0.791095890410959\n",
      "L1 Recall:  0.8783269961977186\n",
      "L1 F1-Score:  0.8324324324324324\n",
      "\n",
      "L2 Accuracy:  0.7546728971962616\n",
      "L2 Precision:  0.5367647058823529\n",
      "L2 Recall:  0.6347826086956522\n",
      "L2 F1-Score:  0.5816733067729084\n",
      "\n",
      "L3 Accuracy:  0.9462616822429907\n",
      "L3 Recall:  0.0\n",
      "\n",
      "L4 Accuracy:  0.9369158878504673\n",
      "L4 Recall:  0.0\n",
      "\n",
      "Mirco Accuracy:  0.8551401869158879\n",
      "Mirco Precision:  0.7102803738317757\n",
      "Mirco Recall:  0.7102803738317757\n",
      "Mirco F1-Score:  0.7102803738317757\n"
     ]
    }
   ],
   "source": [
    "# Name: Muhammad Athar\n",
    "# Roll #: 18I-0692\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import math\n",
    "import copy\n",
    "                                       # K-Nearest Neighbours (KNN) Classification\n",
    "\n",
    "# Dataset: Car Evaluation\n",
    "# Classes: 4 (unacc, acc, good, vgood)\n",
    "# Features / Attributes: 6\n",
    "# Records: 1728\n",
    "\n",
    "def keyfunc(s):\n",
    "    return s[1]\n",
    "\n",
    "def find_distance_with_all_neighbours(val_or_test_dataset_record, train_dataset_record):\n",
    "    sum_of_squares = 0\n",
    "    euclidean_dist = 0\n",
    "    \n",
    "    for k in range(0, 6):\n",
    "        diff = train_dataset_record[k] - val_or_test_dataset_record[k]\n",
    "        \n",
    "        square = diff ** 2\n",
    "        \n",
    "        sum_of_squares = sum_of_squares + square\n",
    "        \n",
    "    euclidean_dist = math.sqrt(sum_of_squares)\n",
    "    \n",
    "    return euclidean_dist\n",
    "        \n",
    "def find_accuracy_with_some_k(k, original_dataset_numerical, train_dataset_numerical, validation_dataset_numerical):\n",
    "    euc_distance_with_all_neighbours = [0 for i in range(len(train_dataset_numerical))]\n",
    "    \n",
    "    least_distant_neigh_indices = [0 for i in range(k)]\n",
    "    \n",
    "    car_classes = [0, 0, 0, 0]\n",
    "    \n",
    "    accuracy = 0\n",
    "    \n",
    "    matched = 0\n",
    "    not_matched = 0\n",
    "    \n",
    "    # Predicting Classes of each record of Validation Dataset\n",
    "    for i in range(0, len(validation_dataset_numerical)):\n",
    "        for j in range(0, len(train_dataset_numerical)):\n",
    "            euc_distance_with_all_neighbours[j] = find_distance_with_all_neighbours(validation_dataset_numerical[i], train_dataset_numerical[j])\n",
    "        \n",
    "        enu_euc_dist = enumerate(euc_distance_with_all_neighbours)\n",
    "        \n",
    "        enu_euc_dist = list(enu_euc_dist)\n",
    "        \n",
    "        result = sorted(enu_euc_dist, key=keyfunc)\n",
    "        \n",
    "        for m in range(0, k):\n",
    "            least_distant_neigh_indices[m] = result[m][0]\n",
    "        \n",
    "        car_classes = [0, 0, 0, 0]\n",
    "        \n",
    "        for n in range(k):\n",
    "            if (train_dataset_numerical[least_distant_neigh_indices[n]][6] == 1):\n",
    "                car_classes[0] = car_classes[0] + 1   \n",
    "            elif train_dataset_numerical[least_distant_neigh_indices[n]][6] == 2:\n",
    "                car_classes[1] = car_classes[1] + 1\n",
    "            elif train_dataset_numerical[least_distant_neigh_indices[n]][6] == 3:\n",
    "                car_classes[2] = car_classes[2] + 1\n",
    "            elif train_dataset_numerical[least_distant_neigh_indices[n]][6] == 4:\n",
    "                car_classes[3] = car_classes[3] + 1\n",
    "        \n",
    "        car_classes_enumerate = enumerate(car_classes)\n",
    "        \n",
    "        car_classes_enumerate = list(car_classes_enumerate)\n",
    "            \n",
    "        result1 = sorted(car_classes_enumerate, key=keyfunc)\n",
    "        \n",
    "        which_class = result1[3][0]\n",
    "        \n",
    "        validation_dataset_numerical[i][6] = which_class + 1\n",
    "    \n",
    "    xy = 1000\n",
    "    \n",
    "    for i in range(0, len(validation_dataset_numerical)):\n",
    "        if original_dataset_numerical[xy][6] == validation_dataset_numerical[i][6]:\n",
    "            matched = matched + 1\n",
    "            xy+=1\n",
    "        else:\n",
    "            not_matched = not_matched + 1\n",
    "            xy+=1\n",
    "    \n",
    "    accuracy = (matched / len(validation_dataset_numerical)) * 100\n",
    "                                         \n",
    "    return accuracy\n",
    "    \n",
    "def change_categorical_values_to_numerical(dataset):\n",
    "    dataset_len = len(dataset)\n",
    "    \n",
    "    for i in range(0, dataset_len):\n",
    "        for j in range(0, 7):\n",
    "            # First Column - buying\n",
    "            if j == 0:\n",
    "                if dataset[i][j] == 'vhigh':\n",
    "                    dataset[i][j] = 4\n",
    "                elif dataset[i][j] == 'high':\n",
    "                    dataset[i][j] = 3\n",
    "                elif dataset[i][j] == 'med':\n",
    "                    dataset[i][j] = 2\n",
    "                elif dataset[i][j] == 'low':\n",
    "                    dataset[i][j] = 1\n",
    "            \n",
    "            # 2nd Column - maint\n",
    "            elif j == 1:\n",
    "                if dataset[i][j] == 'vhigh':\n",
    "                    dataset[i][j] = 4\n",
    "                elif dataset[i][j] == 'high':\n",
    "                    dataset[i][j] = 3\n",
    "                elif dataset[i][j] == 'med':\n",
    "                    dataset[i][j] = 2\n",
    "                elif dataset[i][j] == 'low':\n",
    "                    dataset[i][j] = 1\n",
    "                    \n",
    "            # 3rd Column - doors\n",
    "            elif j == 2:\n",
    "                if dataset[i][j] == '2':\n",
    "                    dataset[i][j] = 2\n",
    "                if dataset[i][j] == '3':\n",
    "                    dataset[i][j] = 3\n",
    "                if dataset[i][j] == '4':\n",
    "                    dataset[i][j] = 4\n",
    "                if dataset[i][j] == '5more':\n",
    "                    dataset[i][j] = 5\n",
    "                    \n",
    "            # 4th Column - persons\n",
    "            elif j == 3:\n",
    "                if dataset[i][j] == '2':\n",
    "                    dataset[i][j] = 2\n",
    "                if dataset[i][j] == '3':\n",
    "                    dataset[i][j] = 3\n",
    "                if dataset[i][j] == '4':\n",
    "                    dataset[i][j] = 4\n",
    "                if dataset[i][j] == 'more':\n",
    "                    dataset[i][j] = 5\n",
    "                    \n",
    "            # 5th Column - lug_boot\n",
    "            elif j == 4:\n",
    "                if dataset[i][j] == 'small':\n",
    "                    dataset[i][j] = 1\n",
    "                elif dataset[i][j] == 'med':\n",
    "                    dataset[i][j] = 2\n",
    "                elif dataset[i][j] == 'big':\n",
    "                    dataset[i][j] = 3\n",
    "                    \n",
    "            # 6th Column - safety\n",
    "            elif j == 5:\n",
    "                if dataset[i][j] == 'low':\n",
    "                    dataset[i][j] = 1\n",
    "                elif dataset[i][j] == 'med':\n",
    "                    dataset[i][j] = 2\n",
    "                elif dataset[i][j] == 'high':\n",
    "                    dataset[i][j] = 3\n",
    "                    \n",
    "            # 7th Column - CLASS\n",
    "            elif j == 6:\n",
    "                if dataset[i][j] == 'unacc':\n",
    "                    dataset[i][j] = 1\n",
    "                elif dataset[i][j] == 'acc':\n",
    "                    dataset[i][j] = 2\n",
    "                elif dataset[i][j] == 'good':\n",
    "                    dataset[i][j] = 3\n",
    "                elif dataset[i][j] == 'vgood':\n",
    "                    dataset[i][j] = 4\n",
    "\n",
    "def find_accuracy_of_test_dataet(confusion_matrix, k, original_dataset_numerical, train_dataset_numerical, test_dataset_numerical):\n",
    "    euc_distance_with_all_neighbours = [0 for i in range(len(train_dataset_numerical))]\n",
    "    \n",
    "    least_distant_neigh_indices = [0 for i in range(k)]\n",
    "    \n",
    "    car_classes = [0, 0, 0, 0]\n",
    "    \n",
    "    accuracy = 0\n",
    "    \n",
    "    matched = 0\n",
    "    not_matched = 0\n",
    "    \n",
    "    # Predicting Classes of each record of Test Dataset\n",
    "    for i in range(0, len(test_dataset_numerical)):\n",
    "        for j in range(0, len(train_dataset_numerical)):\n",
    "            euc_distance_with_all_neighbours[j] = find_distance_with_all_neighbours(test_dataset_numerical[i], train_dataset_numerical[j])\n",
    "        \n",
    "        enu_euc_dist = enumerate(euc_distance_with_all_neighbours)\n",
    "        \n",
    "        enu_euc_dist = list(enu_euc_dist)\n",
    "        \n",
    "        result = sorted(enu_euc_dist, key=keyfunc)\n",
    "        \n",
    "        for m in range(0, k):\n",
    "            least_distant_neigh_indices[m] = result[m][0]\n",
    "        \n",
    "        car_classes = [0, 0, 0, 0]\n",
    "        \n",
    "        for n in range(k):\n",
    "            if (train_dataset_numerical[least_distant_neigh_indices[n]][6] == 1):\n",
    "                car_classes[0] = car_classes[0] + 1   \n",
    "            elif train_dataset_numerical[least_distant_neigh_indices[n]][6] == 2:\n",
    "                car_classes[1] = car_classes[1] + 1\n",
    "            elif train_dataset_numerical[least_distant_neigh_indices[n]][6] == 3:\n",
    "                car_classes[2] = car_classes[2] + 1\n",
    "            elif train_dataset_numerical[least_distant_neigh_indices[n]][6] == 4:\n",
    "                car_classes[3] = car_classes[3] + 1\n",
    "        \n",
    "        car_classes_enumerate = enumerate(car_classes)\n",
    "        \n",
    "        car_classes_enumerate = list(car_classes_enumerate)\n",
    "            \n",
    "        result1 = sorted(car_classes_enumerate, key=keyfunc)\n",
    "        \n",
    "        which_class = result1[3][0]\n",
    "        \n",
    "        test_dataset_numerical[i][6] = which_class + 1\n",
    "    \n",
    "    xy = 1000\n",
    "    \n",
    "    for i in range(0, len(test_dataset_numerical)):\n",
    "        org_class = original_dataset_numerical[xy][6] - 1\n",
    "        predicted_class = test_dataset_numerical[i][6] - 1\n",
    "        \n",
    "        confusion_matrix[predicted_class][org_class]+=1\n",
    "        \n",
    "        if original_dataset_numerical[xy][6] == test_dataset_numerical[i][6]:\n",
    "            matched = matched + 1\n",
    "            xy+=1\n",
    "        else:\n",
    "            not_matched = not_matched + 1\n",
    "            xy+=1\n",
    "    \n",
    "    accuracy = (matched / len(test_dataset_numerical)) * 100\n",
    "                                         \n",
    "    return accuracy\n",
    "    \n",
    "def load_and_split_dataset(dataset_fname, original_dataset=[], train=[], validation=[], test=[]):\n",
    "    \n",
    "    # We have 1728 records. We will give 1000 records to Train Dataset, 300 records to Validation dataset and 428 records\n",
    "    # to Test Dataset.\n",
    "    \n",
    "    # Reading car.data dataset file in text mode.\n",
    "    with open(dataset_fname, 'r') as csvfile:\n",
    "        lines = csv.reader(csvfile)\n",
    "        \n",
    "        org_dataset = list(lines)\n",
    "        \n",
    "        for i in range(0, len(org_dataset)):\n",
    "            if i < 1000:\n",
    "                train.append(copy.deepcopy(org_dataset[i]))\n",
    "                original_dataset.append(copy.deepcopy(org_dataset[i]))\n",
    "            if i >= 1000 and i < 1300:\n",
    "                validation.append(copy.deepcopy(org_dataset[i]))\n",
    "                original_dataset.append(copy.deepcopy(org_dataset[i]))\n",
    "            if i >= 1300 and i < 1728:\n",
    "                test.append(copy.deepcopy(org_dataset[i]))\n",
    "                original_dataset.append(copy.deepcopy(org_dataset[i]))\n",
    "                \n",
    "original_dataset = []\n",
    "\n",
    "# Original Dataset divided to 3 parts i.e. Train Dataset, Validation Dataset, Test Dataset\n",
    "train_dataset = []\n",
    "validation_dataset = []\n",
    "test_dataset = []\n",
    "\n",
    "load_and_split_dataset('Assignment3/car.data', original_dataset, train_dataset, validation_dataset, test_dataset)\n",
    "\n",
    "original_dataset_numerical = []\n",
    "train_dataset_numerical = []\n",
    "validation_dataset_numerical = []\n",
    "test_dataset_numerical = []\n",
    "\n",
    "for i in range(0, len(original_dataset)):\n",
    "        original_dataset_numerical.append(copy.deepcopy(original_dataset[i]))\n",
    "        \n",
    "for i in range(0, len(train_dataset)):\n",
    "        train_dataset_numerical.append(copy.deepcopy(train_dataset[i]))\n",
    "\n",
    "for i in range(0, len(validation_dataset)):\n",
    "        validation_dataset_numerical.append(copy.deepcopy(validation_dataset[i]))\n",
    "\n",
    "for i in range(0, len(test_dataset)):\n",
    "        test_dataset_numerical.append(copy.deepcopy(test_dataset[i]))\n",
    "\n",
    "# Changing Categorical Data to Numerical Data\n",
    "change_categorical_values_to_numerical(original_dataset_numerical)\n",
    "change_categorical_values_to_numerical(train_dataset_numerical)\n",
    "change_categorical_values_to_numerical(validation_dataset_numerical)\n",
    "change_categorical_values_to_numerical(test_dataset_numerical)\n",
    "\n",
    "for i in range(0, len(validation_dataset)):\n",
    "    validation_dataset_numerical[i][6] = -1\n",
    "\n",
    "for i in range(0, len(test_dataset)):\n",
    "    test_dataset_numerical[i][6] = -1\n",
    "\n",
    "flag = True\n",
    "\n",
    "previous_accuracy = 0\n",
    "kk = -1\n",
    "val_dataset_accuracy_ = -10\n",
    "confusion_matrix = [[0 for i in range(4)] for j in range(4)]\n",
    "\n",
    "while flag == True:\n",
    "    kk+=2\n",
    "    \n",
    "    val_dataset_accuracy_ = find_accuracy_with_some_k(kk, original_dataset_numerical, train_dataset_numerical, validation_dataset_numerical)\n",
    "    \n",
    "    if val_dataset_accuracy_ > previous_accuracy:\n",
    "        flag = True\n",
    "        previous_accuracy = val_dataset_accuracy_\n",
    "    else:\n",
    "        flag = False\n",
    "\n",
    "test_dataset_accuracy = find_accuracy_of_test_dataet(confusion_matrix, kk, original_dataset_numerical, train_dataset_numerical, test_dataset_numerical)\n",
    "\n",
    "print(\"Tuned K Value: \", kk)\n",
    "print(\"\\nValidation Dataset Accuray: \", val_dataset_accuracy_)\n",
    "print(\"Test Dataset Accuracy: \", test_dataset_accuracy)\n",
    "\n",
    "# For Label 1\n",
    "l1_TP = confusion_matrix[0][0]\n",
    "l1_TN = confusion_matrix[1][1] + confusion_matrix[2][1] + confusion_matrix[3][1] + confusion_matrix[1][2] + confusion_matrix[2][2] + confusion_matrix[3][2] + confusion_matrix[1][3] + confusion_matrix[2][3] + confusion_matrix[3][3]\n",
    "l1_FP = confusion_matrix[0][1] + confusion_matrix[0][2] + confusion_matrix[0][3]\n",
    "l1_FN = confusion_matrix[1][0] + confusion_matrix[2][0] + confusion_matrix[3][0]\n",
    "\n",
    "l1_accuracy = (l1_TP + l1_TN) / (l1_TP + l1_TN + l1_FP + l1_FN)\n",
    "l1_precision = l1_TP / (l1_TP + l1_FP)\n",
    "l1_recall = l1_TP / (l1_TP + l1_FN)\n",
    "l1_f1_score = 2 * ((l1_precision * l1_recall) / (l1_precision + l1_recall))\n",
    "\n",
    "print(\"\\nL1 Accuracy: \", l1_accuracy)\n",
    "print(\"L1 Precision: \", l1_precision)\n",
    "print(\"L1 Recall: \", l1_recall)\n",
    "print(\"L1 F1-Score: \", l1_f1_score)\n",
    "\n",
    "# For Label 2\n",
    "l2_TP = confusion_matrix[1][1]\n",
    "l2_TN = confusion_matrix[0][0] + confusion_matrix[0][2] + confusion_matrix[0][3] + confusion_matrix[2][0] + confusion_matrix[2][2] + confusion_matrix[2][3] + confusion_matrix[3][0] + confusion_matrix[3][2] + confusion_matrix[3][3]\n",
    "l2_FP = confusion_matrix[1][0] + confusion_matrix[1][2] + confusion_matrix[1][3]\n",
    "l2_FN = confusion_matrix[0][1] + confusion_matrix[2][1] + confusion_matrix[3][1]\n",
    "\n",
    "l2_accuracy = (l2_TP + l2_TN) / (l2_TP + l2_TN + l2_FP + l2_FN)\n",
    "l2_precision = l2_TP / (l2_TP + l2_FP)\n",
    "l2_recall = l2_TP / (l2_TP + l2_FN)\n",
    "l2_f1_score = 2 * ((l2_precision * l2_recall) / (l2_precision + l2_recall))\n",
    "\n",
    "print(\"\\nL2 Accuracy: \", l2_accuracy)\n",
    "print(\"L2 Precision: \", l2_precision)\n",
    "print(\"L2 Recall: \", l2_recall)\n",
    "print(\"L2 F1-Score: \", l2_f1_score)\n",
    "\n",
    "# For Label 3\n",
    "l3_TP = confusion_matrix[2][2]\n",
    "l3_TN = confusion_matrix[0][0] + confusion_matrix[0][1] + confusion_matrix[0][3] + confusion_matrix[1][0] + confusion_matrix[1][1] + confusion_matrix[1][3] + confusion_matrix[3][0] + confusion_matrix[3][1] + confusion_matrix[3][3]\n",
    "l3_FP = confusion_matrix[2][0] + confusion_matrix[2][1] + confusion_matrix[2][3]\n",
    "l3_FN = confusion_matrix[0][2] + confusion_matrix[1][2] + confusion_matrix[3][2]\n",
    "\n",
    "l3_accuracy = (l3_TP + l3_TN) / (l3_TP + l3_TN + l3_FP + l3_FN)\n",
    "#l3_precision = l3_TP / (l3_TP + l3_FP)\n",
    "l3_recall = l3_TP / (l3_TP + l3_FN)\n",
    "#l3_f1_score = 2 * ((l3_precision * l3_recall) / (l3_precision + l3_recall))\n",
    "\n",
    "print(\"\\nL3 Accuracy: \", l3_accuracy)\n",
    "print(\"L3 Recall: \", l3_recall)\n",
    "#print(\"L3 Precision: \", l3_precision)\n",
    "#print(\"L3 F1_Score: \", l3_f1_score)\n",
    "\n",
    "# For label 4\n",
    "l4_TP = confusion_matrix[3][3]\n",
    "l4_TN = confusion_matrix[0][0] + confusion_matrix[0][1] + confusion_matrix[0][2] + confusion_matrix[1][0] + confusion_matrix[1][1] + confusion_matrix[1][2] + confusion_matrix[2][0] + confusion_matrix[2][1] + confusion_matrix[2][2]\n",
    "l4_FP = confusion_matrix[3][0] + confusion_matrix[3][1] + confusion_matrix[3][2]\n",
    "l4_FN = confusion_matrix[0][3] + confusion_matrix[1][3] + confusion_matrix[2][3]\n",
    "\n",
    "l4_accuracy = (l4_TP + l4_TN) / (l4_TP + l4_TN + l4_FP + l4_FN)\n",
    "#l4_precision = l4_TP / (l4_TP + l4_FP)\n",
    "l4_recall = l4_TP / (l4_TP + l4_FN)\n",
    "#l4_f1_score = 2 * ((l4_precision * l4_recall) / (l4_precision + l4_recall))\n",
    "\n",
    "print(\"\\nL4 Accuracy: \", l4_accuracy)\n",
    "print(\"L4 Recall: \", l4_recall)\n",
    "#print(\"L4 Precision: \", l4_precision)\n",
    "#print(\"L4 F1_Score: \", l4_f1_score)\n",
    "\n",
    "# Mirco - Macro\n",
    "\n",
    "total_tp = l1_TP + l2_TP + l3_TP + l4_TP\n",
    "total_tn = l1_TN + l2_TN + l3_TN + l4_TN\n",
    "total_fp = l1_FP + l2_FP + l3_FP + l4_FP\n",
    "total_fn = l1_FN + l2_FN + l3_FN + l4_FN\n",
    "\n",
    "micro_accuracy = (total_tp + total_tn) / (total_tp + total_tn + total_fp + total_fn)\n",
    "micro_precision = total_tp / (total_tp + total_fp)\n",
    "micro_recall = total_tp / (total_tp + total_fn)\n",
    "micro_f1_score = 2 * ((micro_precision * micro_recall) / (micro_precision + micro_recall))\n",
    "\n",
    "print(\"\\nMirco Accuracy: \", micro_accuracy)\n",
    "print(\"Mirco Precision: \", micro_precision)\n",
    "print(\"Mirco Recall: \", micro_recall)\n",
    "print(\"Mirco F1-Score: \", micro_f1_score)\n",
    "\n",
    "#macro_f1_score = (l1_f1_score + l2_f1_score + l3_f1_score + l4_f1_score) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
